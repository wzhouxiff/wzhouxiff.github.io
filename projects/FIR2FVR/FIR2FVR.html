<!DOCTYPE html
  PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Analysis and Benchmarking of Extending Blind Face Image Restoration to Videos
  </title>
  <!--=================Meta tags==========================-->
  <meta name="robots" content="index,follow">
  <meta name="description"
    content="In this work, we first present a fair evaluation benchmark, in which we first introduce a Real-world Low-Quality Face Video benchmark (RFV-LQ), evaluate several leading image-based face restoration algorithms, and conduct a thorough systematical analysis of the benefits and challenges associated with extending blind face image restoration algorithms to degraded face videos.">
  <meta name="keywords"
    content="blind, face image restoration, face video restoration, systematic anaylese">
  <link rel="author" href="https://github.com/wzhouxiff">
  <!--=================js==========================-->
  <link href="./css.css" rel="stylesheet" type="text/css">
  <link rel="stylesheet" type="text/css" href="./project.css" media="screen">
  <script src="./effect.js "></script>
  <!-- Latex -->
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
        TeX: { equationNumbers: { autoNumber: "AMS" } },
      });
      </script>
  <script type="text/javascript" async
    src="js/MathJax.js">
    </script>
  <!--=================Google Analytics==========================-->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-129775907-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }
    gtag('js', new Date());

    gtag('config', 'UA-129775907-1');
  </script>
</head>

<body>
  <div id="content">
    <div id="content-inner">
      <div class="section head">
        <h1>
          <font color="Tomato">FIR2FVR</font>: Analysis and Benchmarking of Extending Blind 
        </h1>
        <h1>
          <font color="Tomato">F</font>ace <font color="Tomato">I</font>mage <font color="Tomato">R</font>estoration to <font color="Tomato">V</font>ideo
        </h1>
        <!--=================Authors==========================-->
        <div class="authors">
          <a href="https://github.com/wzhouxiff" target="_blank">Zhouxia Wang<sup>1</sup></a> &nbsp;&nbsp;&nbsp;&nbsp;
          <a href="https://scholar.google.com/citations?hl=zh-CN&user=0GTpIAIAAAAJ&view_op=list_works&sortby=pubdate" target="_blank">Jiawei Zhang<sup>3</sup></a> &nbsp;&nbsp;&nbsp;&nbsp;
          <a href="https://xinntao.github.io/" target="_blank">Xintao Wang<sup>2</sup></a> &nbsp;&nbsp;&nbsp;&nbsp;
          <a href="http://tianshuichen.com/" target="_blank">Tianshui Chen<sup>4</sup></a> &nbsp;&nbsp;&nbsp;&nbsp;
          <a href="https://scholar.google.com/citations?user=4oXBp9UAAAAJ&hl=en" target="_blank">Ying Shan<sup>2</sup></a> &nbsp;&nbsp;&nbsp;&nbsp;
          <a href="https://engineering.tamu.edu/cse/profiles/Wang-Wenping.html" target="_blank">Wenping Wang<sup>1</sup></a> &nbsp;&nbsp;&nbsp;&nbsp;
          <a href="http://luoping.me/" target="_blank">Ping Luo<sup>1</sup></a>
        </div>

        <div class="affiliations ">
          <sup>1</sup>The University of Hong Kong,
          <sup>2</sup>ARC Lab, Tencent PCG,
          <sup>3</sup>SenseTime Research,
          <sup>4</sup>Guangdong University of Technology
        </div>
        <!--=================Tabs==========================-->
        <ul id="tabs">
          <li><a href="#materials" name="#tab1">Materials</a></li>
          <!-- <li><a href="#results" name="#tab4">Results</a></li> -->
          <li><a href="#citation" name="#tab5">Citation</a></li>
          <li><a href="#agreement" name="#tab5">Agreement</a></li>
      </div>
      <br>
      <!--=================Teasers==========================-->
      <div id="img_intro_examples" class="img_container">
        <center>
          <div class="leftView">
            <div class="mask" style="width:80px;height:80px"></div>
            <!-- <img class='small' src="./GFPGAN_src/gfpgan_teaser.jpg"> -->
            <iframe width="872" height="491" src="https://www.youtube.com/embed/O13u_5XP9zY" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
          </div>
        </center>
      </div>
      <!-- <div class="section">
        <p>Comparisons with state-of-the-art face restoration methods: HiFaceGAN, DFDNet, Wan et al.
          and
          PULSE on the real-world low-quality images. While previous methods struggle to restore faithful facial
          details or
          retain
          face identity, our proposed GFP-GAN achieves a good balance of realness and fidelity with much less artifacts.
          In
          addition,
          the powerful generative facial prior allows us to perform restoration and color enhancement jointly.
        </p>
      </div> -->

      <!--=================Abstract==========================-->
      <div class="section abstract">
        <h2>Abstract</h2>
        <br>
        <p>
          Recent progress in blind face restoration has resulted in producing high-quality restored results for static images. However, efforts to extend these advancements to video scenarios have been minimal, partly because of the absence of benchmarks that allow for a comprehensive and fair comparison. 
          In this work, we first present a fair evaluation benchmark, in which we first introduce a Real-world Low-Quality Face Video benchmark (RFV-LQ), evaluate several leading image-based face restoration algorithms, and conduct a thorough systematical analysis of the benefits and challenges associated with extending blind face image restoration algorithms to degraded face videos. 
          Our analysis identifies several key issues, primarily categorized into two aspects: significant jitters in facial components and noise-shape flickering between frames. 
          To address these issues, we propose a Temporal Consistency Network (TCN) cooperated with alignment smoothing to reduce jitters and flickers in restored videos. 
          TCN is a flexible component that can be seamlessly plugged into the most advanced face image restoration algorithms, ensuring the quality of image-based restoration is maintained as closely as possible. 
          Extensive experiments have been conducted to evaluate the effectiveness and efficiency of our proposed TCN and alignment smoothing operation.
        </p>
      </div>
      <!--=================Materials==========================-->
      <div class="section materials" , id="materials">
        <h2>Materials</h2>
        <table width="100%" align="center" border=none cellspacing="0" cellpadding="30">
          <tr>
            <td width="50%">
              <center>
                <!-- <a href="https://arxiv.org/abs/2101.04061" target="_blank" class="imageLink"><img
                    src="./src/paper_thumbnail.png" , width="80%"></a><br><br>
                <a href="https://arxiv.org/abs/2101.04061" target="_blank">Paper</a> -->
                <a href="https://ieeexplore.ieee.org/abstract/document/10693312" target="_blank" class="imageLink"><img
                  src="./src/paper_thumbnail.png" , width="80%"></a><br><br>
              <a href="https://ieeexplore.ieee.org/abstract/document/10693312" target="_blank">Paper</a>
              </center>
            </td>
            <!-- <td width="40%" valign="middle">
              <center>
                <a href="https://github.com/TencentARC/GFPGAN" target="_blank" class="imageLink"><img
                    src="./icon_github.png" , width="50%"></a><br><br>
                <a href="https://github.com/TencentARC/GFPGAN" target="_blank">Codes</a>
              </center>
            </td> -->
            <td width="50%" valign="middle">
              <center>
                <a href="https://entuedu-my.sharepoint.com/:u:/g/personal/zhouxia_wang_staff_main_ntu_edu_sg/EaOlYlboL9xOiivC3I8wN2YBLoDH6HoP81nAYYU_EiZu8Q?e=ooCNLV" target="_blank" class="imageLink"><img
                    src="./folders.png" , width="50%"></a><br><br>
                <a href="https://entuedu-my.sharepoint.com/:u:/g/personal/zhouxia_wang_staff_main_ntu_edu_sg/EaOlYlboL9xOiivC3I8wN2YBLoDH6HoP81nAYYU_EiZu8Q?e=ooCNLV" target="_blank">Dataset: RFV-LQ (OneDrive)</a>
              </center>
            </td>
          </tr>
        </table>
      </div>
      <!-- <div class="section materials" , id="materials">
        <h2>Real-world Low-Quality Face Video benchmark (RFV-LQ)</h2>
        <table width="100%" align="center" border=none cellspacing="0" cellpadding="30">
          <tr>
            <td width="50%">
              <center>
                <br>
           <img src="./folders.png" , width="30%"><br><br>
            <span class="block-text">
            <a href="https://entuedu-my.sharepoint.com/:u:/g/personal/zhouxia_wang_staff_main_ntu_edu_sg/EaOlYlboL9xOiivC3I8wN2YBLoDH6HoP81nAYYU_EiZu8Q?e=ooCNLV" target="_blank"> <strong>&nbsp; RFV-LQ (OneDrive)&nbsp;</strong></a></span><br><br>
          <span class=" block-text">
            <a href="https://share.weiyun.com/EwHMw16Y" target="_blank"> <strong>&nbsp;CelebChild-Test (腾讯微云)&nbsp;</strong></a></span>
              </center>
            </td>
            <td width="50%" valign="middle">
              <br>
              <center>
           <img src="./folders.png" , width="30%"><br><br>
            <span class="block-text">
            <a href="https://1drv.ms/u/s!AkGVnRhFUbx2hLUHkv8QhhLwD8yd1g?e=ecNWSZ" target="_blank"> <strong>&nbsp;WebPhoto-Test (OneDrive)&nbsp;</strong></a></span><br><br>
          <span class=" block-text">
            <a href="https://share.weiyun.com/aZH010rK" target="_blank"> <strong>&nbsp;WebPhoto-Test (腾讯微云)&nbsp;</strong></a></span>
              </center>
            </td>
          </tr>
          <tr>
            <td width="50%">
              <center>
                <br>
           <img src="./folders.png" , width="30%"><br><br>
            <span class="block-text">
            <a href="https://1drv.ms/u/s!AkGVnRhFUbx2hZ4zsVomiSPubMEsrA?e=GwhEao" target="_blank"> <strong>&nbsp;CelebA-Test (LQ) (OneDrive)&nbsp;</strong></a></span><br><br>
          <span class=" block-text">
            <a href="https://share.weiyun.com/2oHHyb0k" target="_blank"> <strong>&nbsp;CelebA-Test (LQ) (腾讯微云)&nbsp;</strong></a></span>
              </center>
            </td>
            <td width="50%" valign="middle">
              <br>
              <center>
           <img src="./folders.png" , width="30%"><br><br>
            <span class="block-text">
            <a href="https://1drv.ms/u/s!AkGVnRhFUbx2hZ40v049sYcF34hc0A?e=HxGEdl" target="_blank"> <strong>&nbsp;CelebA-Test (HQ) (OneDrive)&nbsp;</strong></a></span><br><br>
          <span class=" block-text">
            <a href="https://share.weiyun.com/bFFjCh22" target="_blank"> <strong>&nbsp;CelebA-Test (HQ) (腾讯微云)&nbsp;</strong></a></span>
              </center>
            </td>
          </tr>
          <tr>
            <td width="50%">
              <center>
                <br>
           <img src="./folders.png" , width="30%"><br><br>
            <span class="block-text">
            <a href="https://1drv.ms/u/s!AkGVnRhFUbx2hZ4yb_Ly5NTopkyY8w?e=abk8bf" target="_blank"> <strong>&nbsp;LFW-Test (OneDrive)&nbsp;</strong></a></span><br><br>
          <span class=" block-text">
            <a href="https://share.weiyun.com/cQ3gRylG" target="_blank"> <strong>&nbsp;LFW-Test (腾讯微云)&nbsp;</strong></a></span>
              </center>
            </td>
            <td width="50%" valign="middle">

            </td>
          </tr>
        </table>
      </div> -->

      <!--=================Citation==========================-->
      <div class="section agreement" , id="agreement">
        <h2>Agreement</h2>
        <div class="section bibtex">
          <ul>
            <li>The RFV-LQ dataset is only available to download for non-commercial research purposes. The copyright remains with the original owners of the video.</li>
            <li>All videos of the RFV-LQ dataset are obtained from the Internet which are not property of our institutions. Our institution are not responsible for the content nor the meaning of these videos.</li>
            <!-- <li>You agree not to reproduce, duplicate, copy, sell, trade, resell or exploit for any commercial purposes, any portion of the videos and any portion of derived data. You agree not to further copy, publish or distribute any portion of the RFV-LQ dataset.</li> -->
            <li>The distribution of identities in the RFV-LQ datasets may not be representative of the global human population. Please be careful of unintended societal, gender, racial and other biases when using this dataset.</li>
          </ul>
        </div>
      </div>

      <!--=================Citation==========================-->
      <div class="section citation" , id="citation">
        <h2>Citation</h2>
        <div class="section bibtex">
          <pre>
@ARTICLE{10693312,
author={Wang, Zhouxia and Zhang, Jiawei and Wang, Xintao and Chen, Tianshui and Shan, Ying and Wang, Wenping and Luo, Ping},
journal={IEEE Transactions on Image Processing}, 
title={Analysis and Benchmarking of Extending Blind Face Image Restoration to Videos}, 
year={2024},
volume={33},
number={},
pages={5676-5687},
doi={10.1109/TIP.2024.3463414}}
          </pre>
        </div>
      </div>
      <!--=================Contact==========================-->
      <div class="section contact">
        <h2 id="contact">Contact</h2>
        <p>If you have any question, please contact Zhouxia Wang at <strong>zhouzi1212@gmail.com</strong>.</p>
      </div>
</body>

</html>
